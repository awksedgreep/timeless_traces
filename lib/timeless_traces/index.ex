defmodule TimelessTraces.Index do
  @moduledoc false

  use GenServer

  @default_limit 100
  @default_offset 0

  # Flush pending index operations after this interval
  @index_flush_interval 100

  # Flush SQLite persistence after this interval
  @sqlite_flush_interval 500

  # Batch INSERT up to 400 terms per statement (800 params, under SQLite's 999 limit)
  @terms_batch_size 400

  # Batch INSERT OR REPLACE up to 100 traces per statement (6 params each = 600)
  @trace_batch_size 100

  @blocks_table :timeless_traces_blocks
  @term_index_table :timeless_traces_term_index
  @trace_index_table :timeless_traces_trace_index

  @spec start_link(keyword()) :: GenServer.on_start()
  def start_link(opts) do
    GenServer.start_link(__MODULE__, opts, name: __MODULE__)
  end

  @spec index_block(TimelessTraces.Writer.block_meta(), [String.t()], [tuple()]) :: :ok
  def index_block(block_meta, terms, trace_rows) do
    GenServer.call(__MODULE__, {:index_block, block_meta, terms, trace_rows})
  end

  @spec index_block_async(TimelessTraces.Writer.block_meta(), [String.t()], [tuple()]) :: :ok
  def index_block_async(block_meta, terms, trace_rows) do
    GenServer.cast(__MODULE__, {:index_block, block_meta, terms, trace_rows})
  end

  # --- Lock-free read functions (run in caller's process) ---

  @spec query(keyword()) :: {:ok, TimelessTraces.Result.t()}
  def query(filters) do
    {search_filters, pagination} = split_pagination(filters)
    {term_filters, time_filters} = split_filters(search_filters)
    order = Keyword.get(pagination, :order, :desc)
    block_ids = find_matching_blocks_ets(term_filters, time_filters, order)
    storage = :persistent_term.get({__MODULE__, :storage})

    do_query_parallel(block_ids, storage, pagination, search_filters)
  end

  @spec trace(String.t()) :: {:ok, [TimelessTraces.Span.t()]}
  def trace(trace_id) do
    block_ids = trace_block_ids(trace_id)
    storage = :persistent_term.get({__MODULE__, :storage})

    block_info =
      block_ids
      |> MapSet.to_list()
      |> Enum.flat_map(fn bid ->
        case :ets.lookup(@blocks_table, bid) do
          [{^bid, file_path, _byte_size, _entry_count, _ts_min, _ts_max, format, _created_at}] ->
            [{bid, file_path, format}]

          [] ->
            []
        end
      end)

    do_trace_parallel(block_info, storage, trace_id)
  end

  @spec stats() :: {:ok, TimelessTraces.Stats.t()}
  def stats do
    db_path = :persistent_term.get({__MODULE__, :db_path})

    rows = :ets.tab2list(@blocks_table)

    {total_blocks, total_entries, total_bytes, oldest, newest, format_stats} =
      Enum.reduce(
        rows,
        {0, 0, 0, nil, nil, %{}},
        fn {_bid, _fp, byte_size, entry_count, ts_min, ts_max, format, _created_at},
           {blocks, entries, bytes, old, new, fstats} ->
          new_old = if old == nil or ts_min < old, do: ts_min, else: old
          new_new = if new == nil or ts_max > new, do: ts_max, else: new

          fmt_key = Atom.to_string(format)
          cur = Map.get(fstats, fmt_key, %{blocks: 0, bytes: 0, entries: 0})

          updated = %{
            blocks: cur.blocks + 1,
            bytes: cur.bytes + byte_size,
            entries: cur.entries + entry_count
          }

          {blocks + 1, entries + entry_count, bytes + byte_size, new_old, new_new,
           Map.put(fstats, fmt_key, updated)}
        end
      )

    index_size =
      case db_path do
        nil ->
          0

        path ->
          case File.stat(path) do
            {:ok, %{size: size}} -> size
            _ -> 0
          end
      end

    {:ok,
     %TimelessTraces.Stats{
       total_blocks: total_blocks,
       total_entries: total_entries,
       total_bytes: total_bytes,
       oldest_timestamp: oldest,
       newest_timestamp: newest,
       disk_size: total_bytes,
       index_size: index_size,
       raw_blocks: format_stats["raw"][:blocks] || 0,
       raw_bytes: format_stats["raw"][:bytes] || 0,
       raw_entries: format_stats["raw"][:entries] || 0,
       zstd_blocks: format_stats["zstd"][:blocks] || 0,
       zstd_bytes: format_stats["zstd"][:bytes] || 0,
       zstd_entries: format_stats["zstd"][:entries] || 0,
       openzl_blocks: format_stats["openzl"][:blocks] || 0,
       openzl_bytes: format_stats["openzl"][:bytes] || 0,
       openzl_entries: format_stats["openzl"][:entries] || 0
     }}
  end

  @spec matching_block_ids(keyword()) :: [{integer(), String.t() | nil, :raw | :zstd | :openzl}]
  def matching_block_ids(filters) do
    {search_filters, pagination} = split_pagination(filters)
    {term_filters, time_filters} = split_filters(search_filters)
    order = Keyword.get(pagination, :order, :asc)
    find_matching_blocks_ets(term_filters, time_filters, order)
  end

  @spec raw_block_stats() :: %{
          entry_count: integer(),
          block_count: integer(),
          oldest_created_at: integer() | nil
        }
  def raw_block_stats do
    rows = :ets.tab2list(@blocks_table)

    Enum.reduce(
      rows,
      %{entry_count: 0, block_count: 0, oldest_created_at: nil},
      fn {_bid, _fp, _bs, entry_count, _tsmin, _tsmax, format, created_at}, acc ->
        if format == :raw do
          oldest =
            if acc.oldest_created_at == nil or created_at < acc.oldest_created_at,
              do: created_at,
              else: acc.oldest_created_at

          %{
            entry_count: acc.entry_count + entry_count,
            block_count: acc.block_count + 1,
            oldest_created_at: oldest
          }
        else
          acc
        end
      end
    )
  end

  @spec raw_block_ids() :: [{integer(), String.t() | nil}]
  def raw_block_ids do
    @blocks_table
    |> :ets.tab2list()
    |> Enum.filter(fn {_bid, _fp, _bs, _ec, _tsmin, _tsmax, format, _ca} -> format == :raw end)
    |> Enum.sort_by(fn {_bid, _fp, _bs, _ec, ts_min, _tsmax, _fmt, _ca} -> ts_min end)
    |> Enum.map(fn {bid, fp, _bs, _ec, _tsmin, _tsmax, _fmt, _ca} -> {bid, fp} end)
  end

  @spec distinct_services() :: {:ok, [String.t()]}
  def distinct_services do
    services =
      @term_index_table
      |> :ets.match({:"$1", :_})
      |> List.flatten()
      |> Enum.flat_map(fn
        "service.name:" <> svc -> [svc]
        _ -> []
      end)
      |> Enum.uniq()
      |> Enum.sort()

    {:ok, services}
  end

  @spec distinct_operations(String.t()) :: {:ok, [String.t()]}
  def distinct_operations(service) do
    service_bids = term_block_ids("service.name:#{service}")

    operations =
      @term_index_table
      |> :ets.match({:"$1", :"$2"})
      |> Enum.flat_map(fn
        ["name:" <> name, bid] ->
          if MapSet.member?(service_bids, bid), do: [name], else: []

        _ ->
          []
      end)
      |> Enum.uniq()
      |> Enum.sort()

    {:ok, operations}
  end

  # read_block_data stays as GenServer.call â€” reads BLOB from SQLite (memory mode only)
  @spec read_block_data(integer()) :: {:ok, [map()]} | {:error, term()}
  def read_block_data(block_id) do
    GenServer.call(
      __MODULE__,
      {:read_block_data, block_id},
      TimelessTraces.Config.query_timeout()
    )
  end

  @spec delete_blocks_before(integer()) :: non_neg_integer()
  def delete_blocks_before(cutoff_timestamp) do
    GenServer.call(__MODULE__, {:delete_before, cutoff_timestamp}, 60_000)
  end

  @spec delete_blocks_over_size(non_neg_integer()) :: non_neg_integer()
  def delete_blocks_over_size(max_bytes) do
    GenServer.call(__MODULE__, {:delete_over_size, max_bytes}, 60_000)
  end

  @spec compact_blocks([integer()], TimelessTraces.Writer.block_meta(), [map()]) :: :ok
  def compact_blocks(old_block_ids, new_meta, new_entries) do
    {terms, trace_rows} = precompute(new_entries)

    GenServer.call(
      __MODULE__,
      {:compact_blocks, old_block_ids, new_meta, terms, trace_rows},
      60_000
    )
  end

  @spec backup(String.t()) :: :ok | {:error, term()}
  def backup(target_path) do
    GenServer.call(__MODULE__, {:backup, target_path}, :infinity)
  end

  @spec sync() :: :ok
  def sync, do: GenServer.call(__MODULE__, :sync, TimelessTraces.Config.query_timeout())

  @doc false
  @spec precompute([map()]) :: {[String.t()], [tuple()]}
  def precompute(entries) do
    {terms_set, traces_map} =
      Enum.reduce(entries, {MapSet.new(), %{}}, fn span, {terms_acc, traces_acc} ->
        span_terms = extract_span_terms(span)
        new_terms = Enum.reduce(span_terms, terms_acc, &MapSet.put(&2, &1))

        new_traces =
          Map.update(traces_acc, span.trace_id, [span], fn spans -> [span | spans] end)

        {new_terms, new_traces}
      end)

    terms = MapSet.to_list(terms_set)

    trace_rows =
      Enum.map(traces_map, fn {trace_id, spans} ->
        root = Enum.find(spans, fn s -> s.parent_span_id == nil end) || hd(spans)
        has_error = if Enum.any?(spans, fn s -> s.status == :error end), do: 1, else: 0

        {min_start, max_end, count} =
          Enum.reduce(spans, {nil, nil, 0}, fn s, {mn, mx, c} ->
            {
              if(mn == nil or s.start_time < mn, do: s.start_time, else: mn),
              if(mx == nil or s.end_time > mx, do: s.end_time, else: mx),
              c + 1
            }
          end)

        duration_ns = max_end - min_start
        {trace_id, count, to_string(root.name), duration_ns, has_error}
      end)

    {terms, trace_rows}
  end

  defp extract_span_terms(span) do
    base_terms = [
      "name:#{span.name}",
      "kind:#{span.kind}",
      "status:#{span.status}"
    ]

    service_term =
      case Map.get(span.attributes, "service.name") ||
             Map.get(span.resource || %{}, "service.name") do
        nil -> []
        svc -> ["service.name:#{svc}"]
      end

    attr_terms =
      (span.attributes || %{})
      |> Enum.flat_map(fn
        {"http.method", v} -> ["http.method:#{v}"]
        {"http.status_code", v} -> ["http.status_code:#{v}"]
        {"http.route", v} -> ["http.route:#{v}"]
        {"db.system", v} -> ["db.system:#{v}"]
        {"rpc.system", v} -> ["rpc.system:#{v}"]
        {"messaging.system", v} -> ["messaging.system:#{v}"]
        _ -> []
      end)

    base_terms ++ service_term ++ attr_terms
  end

  # --- GenServer callbacks ---

  @impl true
  def init(opts) do
    storage = Keyword.get(opts, :storage, :disk)

    {db, db_path} =
      case storage do
        :memory ->
          {:ok, db} = Exqlite.Sqlite3.open(":memory:")
          {db, nil}

        :disk ->
          data_dir = Keyword.fetch!(opts, :data_dir)
          db_path = Path.join(data_dir, "index.db")
          {:ok, db} = Exqlite.Sqlite3.open(db_path)
          {db, db_path}
      end

    create_tables(db)

    # Cache prepared statements for hot-path inserts
    {:ok, block_insert_stmt} =
      Exqlite.Sqlite3.prepare(db, """
      INSERT INTO blocks (block_id, file_path, byte_size, entry_count, ts_min, ts_max, data, format)
      VALUES (?1, ?2, ?3, ?4, ?5, ?6, ?7, ?8)
      """)

    {:ok, terms_insert_stmt} = Exqlite.Sqlite3.prepare(db, build_terms_sql(@terms_batch_size))
    {:ok, trace_insert_stmt} = Exqlite.Sqlite3.prepare(db, build_trace_sql(@trace_batch_size))

    # Initialize ETS tables
    init_ets_tables()

    # Store storage mode + db_path in persistent_term
    :persistent_term.put({__MODULE__, :storage}, storage)
    :persistent_term.put({__MODULE__, :db_path}, db_path)

    # Bulk-load from SQLite into ETS
    bulk_load_blocks(db)
    bulk_load_term_index(db)
    bulk_load_trace_index(db)

    {:ok,
     %{
       db: db,
       db_path: db_path,
       storage: storage,
       pending: [],
       flush_timer: nil,
       sqlite_pending: [],
       sqlite_flush_timer: nil,
       block_insert_stmt: block_insert_stmt,
       terms_insert_stmt: terms_insert_stmt,
       trace_insert_stmt: trace_insert_stmt,
       terms_stmt_cache: %{@terms_batch_size => terms_insert_stmt},
       trace_stmt_cache: %{@trace_batch_size => trace_insert_stmt}
     }}
  end

  @impl true
  def terminate(_reason, state) do
    state = flush_pending(state)
    state = flush_sqlite(state)

    Exqlite.Sqlite3.release(state.db, state.block_insert_stmt)

    # Release all cached statements (terms and trace caches include the main stmts)
    for {_n, stmt} <- state.terms_stmt_cache do
      Exqlite.Sqlite3.release(state.db, stmt)
    end

    for {_n, stmt} <- state.trace_stmt_cache do
      Exqlite.Sqlite3.release(state.db, stmt)
    end

    Exqlite.Sqlite3.close(state.db)

    # Clean up persistent_term keys (write-once config only)
    :persistent_term.erase({__MODULE__, :storage})
    :persistent_term.erase({__MODULE__, :db_path})

    # Clean up ETS tables
    safe_delete_ets(@blocks_table)
    safe_delete_ets(@term_index_table)
    safe_delete_ets(@trace_index_table)
  end

  # --- handle_call (grouped) ---

  @impl true
  def handle_call({:index_block, meta, terms, trace_rows}, _from, state) do
    state = flush_pending(state)
    state = flush_sqlite(state)
    result = do_index_block(state, meta, terms, trace_rows)
    state = collect_stmt_cache(state)
    # Update ETS index directly
    insert_block_ets(meta)
    insert_index_entries(terms, trace_rows, meta.block_id)
    {:reply, result, state}
  end

  def handle_call({:delete_before, cutoff}, _from, state) do
    state = flush_pending(state)
    state = flush_sqlite(state)

    {count, deleted_ids, old_terms, old_traces} =
      do_delete_before(state.db, cutoff, state.storage)

    remove_blocks_from_cache(deleted_ids, old_terms, old_traces)
    {:reply, count, state}
  end

  def handle_call({:delete_over_size, max_bytes}, _from, state) do
    state = flush_pending(state)
    state = flush_sqlite(state)

    {count, deleted_ids, old_terms, old_traces} =
      do_delete_over_size(state.db, max_bytes, state.storage)

    remove_blocks_from_cache(deleted_ids, old_terms, old_traces)
    {:reply, count, state}
  end

  def handle_call({:read_block_data, block_id}, _from, state) do
    state = flush_pending(state)
    state = flush_sqlite(state)
    result = read_block_from_db(state.db, block_id)
    {:reply, result, state}
  end

  def handle_call({:compact_blocks, old_ids, new_meta, terms, trace_rows}, _from, state) do
    state = flush_pending(state)
    state = flush_sqlite(state)

    # Collect old terms/traces for cache cleanup
    old_terms = collect_terms_for_blocks(state.db, old_ids)
    old_traces = collect_traces_for_blocks(state.db, old_ids)

    result = do_compact_blocks(state, old_ids, new_meta, terms, trace_rows)
    state = collect_stmt_cache(state)

    # Update cache: remove old, add new
    for id <- old_ids, do: :ets.delete(@blocks_table, id)
    remove_block_ids_from_index(old_ids, old_terms, old_traces)

    insert_block_ets(new_meta)
    insert_index_entries(terms, trace_rows, new_meta.block_id)

    {:reply, result, state}
  end

  def handle_call({:backup, target_path}, _from, state) do
    state = flush_pending(state)
    state = flush_sqlite(state)
    {:ok, stmt} = Exqlite.Sqlite3.prepare(state.db, "VACUUM INTO ?1")
    Exqlite.Sqlite3.bind(stmt, [target_path])
    result = Exqlite.Sqlite3.step(state.db, stmt)
    Exqlite.Sqlite3.release(state.db, stmt)

    case result do
      :done -> {:reply, :ok, state}
      {:error, _} = err -> {:reply, err, state}
    end
  end

  def handle_call(:sync, _from, state) do
    state = flush_pending(state)
    state = flush_sqlite(state)
    {:reply, :ok, state}
  end

  # --- handle_cast ---

  @impl true
  def handle_cast({:index_block, meta, terms, trace_rows}, state) do
    pending = [{meta, terms, trace_rows} | state.pending]
    state = schedule_index_flush(%{state | pending: pending})
    {:noreply, state}
  end

  # --- handle_info ---

  @impl true
  def handle_info(:flush_index, state) do
    state = %{state | flush_timer: nil}
    state = flush_pending(state)
    {:noreply, state}
  end

  def handle_info(:flush_sqlite, state) do
    state = %{state | sqlite_flush_timer: nil}
    state = flush_sqlite(state)
    {:noreply, state}
  end

  # --- ETS initialization ---

  defp init_ets_tables do
    safe_delete_ets(@blocks_table)
    safe_delete_ets(@term_index_table)
    safe_delete_ets(@trace_index_table)

    :ets.new(@blocks_table, [
      :named_table,
      :ordered_set,
      :public,
      read_concurrency: true,
      write_concurrency: true
    ])

    :ets.new(@term_index_table, [
      :named_table,
      :bag,
      :public,
      read_concurrency: true,
      write_concurrency: true
    ])

    :ets.new(@trace_index_table, [
      :named_table,
      :bag,
      :public,
      read_concurrency: true,
      write_concurrency: true
    ])
  end

  defp safe_delete_ets(table) do
    try do
      :ets.delete(table)
    rescue
      ArgumentError -> :ok
    end
  end

  # --- Bulk loading from SQLite ---

  defp bulk_load_blocks(db) do
    {:ok, stmt} =
      Exqlite.Sqlite3.prepare(db, """
      SELECT block_id, file_path, byte_size, entry_count, ts_min, ts_max, format, created_at
      FROM blocks
      """)

    bulk_load_blocks_rows(db, stmt)
    Exqlite.Sqlite3.release(db, stmt)
  end

  defp bulk_load_blocks_rows(db, stmt) do
    case Exqlite.Sqlite3.step(db, stmt) do
      {:row, [block_id, file_path, byte_size, entry_count, ts_min, ts_max, format, created_at]} ->
        :ets.insert(@blocks_table, {
          block_id,
          file_path,
          byte_size,
          entry_count,
          ts_min,
          ts_max,
          to_format_atom(format),
          created_at
        })

        bulk_load_blocks_rows(db, stmt)

      :done ->
        :ok
    end
  end

  defp bulk_load_term_index(db) do
    {:ok, stmt} =
      Exqlite.Sqlite3.prepare(db, "SELECT term, block_id FROM block_terms")

    bulk_load_term_rows(db, stmt)
    Exqlite.Sqlite3.release(db, stmt)
  end

  defp bulk_load_term_rows(db, stmt) do
    case Exqlite.Sqlite3.step(db, stmt) do
      {:row, [term, block_id]} ->
        :ets.insert(@term_index_table, {term, block_id})
        bulk_load_term_rows(db, stmt)

      :done ->
        :ok
    end
  end

  defp bulk_load_trace_index(db) do
    {:ok, stmt} =
      Exqlite.Sqlite3.prepare(db, "SELECT trace_id, block_id FROM trace_index")

    bulk_load_trace_rows(db, stmt)
    Exqlite.Sqlite3.release(db, stmt)
  end

  defp bulk_load_trace_rows(db, stmt) do
    case Exqlite.Sqlite3.step(db, stmt) do
      {:row, [trace_id, block_id]} ->
        :ets.insert(@trace_index_table, {trace_id, block_id})
        bulk_load_trace_rows(db, stmt)

      :done ->
        :ok
    end
  end

  # --- Cache update helpers ---

  defp insert_block_ets(meta) do
    format = Map.get(meta, :format, :zstd)
    created_at = System.system_time(:second)

    :ets.insert(@blocks_table, {
      meta.block_id,
      meta[:file_path],
      meta.byte_size,
      meta.entry_count,
      meta.ts_min,
      meta.ts_max,
      format,
      created_at
    })
  end

  defp insert_index_entries(terms, trace_rows, block_id) do
    if terms != [] do
      term_objects = Enum.map(terms, fn term -> {term, block_id} end)
      :ets.insert(@term_index_table, term_objects)
    end

    if trace_rows != [] do
      trace_objects =
        Enum.map(trace_rows, fn {trace_id, _, _, _, _} -> {trace_id, block_id} end)

      :ets.insert(@trace_index_table, trace_objects)
    end
  end

  # --- Lock-free read helpers ---

  defp term_block_ids(term) do
    @term_index_table
    |> :ets.lookup(term)
    |> Enum.map(fn {_term, bid} -> bid end)
    |> MapSet.new()
  end

  defp trace_block_ids(trace_id) do
    @trace_index_table
    |> :ets.lookup(trace_id)
    |> Enum.map(fn {_trace_id, bid} -> bid end)
    |> MapSet.new()
  end

  defp find_matching_blocks_ets(term_filters, time_filters, order) do
    terms = build_query_terms(term_filters)

    candidate_bids =
      case terms do
        [] ->
          nil

        _ ->
          terms
          |> Enum.map(&term_block_ids/1)
          |> Enum.reduce(&MapSet.intersection/2)
      end

    # Collect all blocks from ETS, apply filters
    all_blocks = :ets.tab2list(@blocks_table)

    filtered =
      all_blocks
      |> Enum.filter(fn {bid, _fp, _bs, _ec, ts_min, ts_max, _fmt, _ca} ->
        term_match =
          case candidate_bids do
            nil -> true
            bids -> MapSet.member?(bids, bid)
          end

        time_match =
          Enum.all?(time_filters, fn
            {:since, ts} -> ts_max >= to_nanos(ts)
            {:until, ts} -> ts_min <= to_nanos(ts)
          end)

        term_match and time_match
      end)
      |> Enum.sort_by(
        fn {_bid, _fp, _bs, _ec, ts_min, _tsmax, _fmt, _ca} -> ts_min end,
        if(order == :asc, do: :asc, else: :desc)
      )
      |> Enum.map(fn {bid, fp, _bs, _ec, _tsmin, _tsmax, fmt, _ca} -> {bid, fp, fmt} end)

    filtered
  end

  # --- Cache cleanup helpers ---

  defp collect_terms_for_blocks(db, block_ids) do
    Enum.flat_map(block_ids, fn id ->
      {:ok, stmt} =
        Exqlite.Sqlite3.prepare(db, "SELECT term FROM block_terms WHERE block_id = ?1")

      Exqlite.Sqlite3.bind(stmt, [id])
      terms = collect_single_column(db, stmt)
      Exqlite.Sqlite3.release(db, stmt)
      Enum.map(terms, fn term -> {term, id} end)
    end)
  end

  defp collect_traces_for_blocks(db, block_ids) do
    Enum.flat_map(block_ids, fn id ->
      {:ok, stmt} =
        Exqlite.Sqlite3.prepare(db, "SELECT trace_id FROM trace_index WHERE block_id = ?1")

      Exqlite.Sqlite3.bind(stmt, [id])
      traces = collect_single_column(db, stmt)
      Exqlite.Sqlite3.release(db, stmt)
      Enum.map(traces, fn trace_id -> {trace_id, id} end)
    end)
  end

  defp remove_block_ids_from_index(_old_ids, old_terms, old_traces) do
    for {term, block_id} <- old_terms do
      :ets.delete_object(@term_index_table, {term, block_id})
    end

    for {trace_id, block_id} <- old_traces do
      :ets.delete_object(@trace_index_table, {trace_id, block_id})
    end
  end

  defp remove_blocks_from_cache(deleted_ids, old_terms, old_traces) do
    if deleted_ids != [] do
      for id <- deleted_ids, do: :ets.delete(@blocks_table, id)
      remove_block_ids_from_index(deleted_ids, old_terms, old_traces)
    end
  end

  # --- Pending flush helpers ---

  defp flush_pending(%{pending: []} = state), do: state

  defp flush_pending(%{pending: pending} = state) do
    items = Enum.reverse(pending)

    # Update ETS index directly (data becomes queryable now)
    for {meta, terms, trace_rows} <- items do
      insert_block_ets(meta)
      insert_index_entries(terms, trace_rows, meta.block_id)
    end

    if state.flush_timer do
      Process.cancel_timer(state.flush_timer)
    end

    # Queue SQLite persistence for background flush
    state = schedule_sqlite_flush(%{state | sqlite_pending: items ++ state.sqlite_pending})
    %{state | pending: [], flush_timer: nil}
  end

  defp flush_sqlite(%{sqlite_pending: []} = state), do: state

  defp flush_sqlite(%{sqlite_pending: pending} = state) do
    items = Enum.reverse(pending)

    Exqlite.Sqlite3.execute(state.db, "BEGIN")

    for {meta, terms, trace_rows} <- items do
      insert_block_data(state, meta, terms, trace_rows)
    end

    Exqlite.Sqlite3.execute(state.db, "COMMIT")

    if state.sqlite_flush_timer do
      Process.cancel_timer(state.sqlite_flush_timer)
    end

    state = collect_stmt_cache(state)
    %{state | sqlite_pending: [], sqlite_flush_timer: nil}
  end

  defp schedule_sqlite_flush(%{sqlite_flush_timer: nil} = state) do
    ref = Process.send_after(self(), :flush_sqlite, @sqlite_flush_interval)
    %{state | sqlite_flush_timer: ref}
  end

  defp schedule_sqlite_flush(state), do: state

  defp collect_stmt_cache(state) do
    # Merge any newly created prepared statements from process dictionary into state
    {new_terms, new_traces} =
      Process.get_keys()
      |> Enum.reduce({state.terms_stmt_cache, state.trace_stmt_cache}, fn
        {:terms_stmt_cache, n}, {tc, trc} ->
          stmt = Process.delete({:terms_stmt_cache, n})
          {Map.put_new(tc, n, stmt), trc}

        {:trace_stmt_cache, n}, {tc, trc} ->
          stmt = Process.delete({:trace_stmt_cache, n})
          {tc, Map.put_new(trc, n, stmt)}

        _, acc ->
          acc
      end)

    %{state | terms_stmt_cache: new_terms, trace_stmt_cache: new_traces}
  end

  defp schedule_index_flush(%{flush_timer: nil} = state) do
    ref = Process.send_after(self(), :flush_index, @index_flush_interval)
    %{state | flush_timer: ref}
  end

  defp schedule_index_flush(state), do: state

  # --- Table creation ---

  defp create_tables(db) do
    Exqlite.Sqlite3.execute(db, "PRAGMA journal_mode=WAL")
    Exqlite.Sqlite3.execute(db, "PRAGMA synchronous=NORMAL")
    Exqlite.Sqlite3.execute(db, "PRAGMA cache_size = -16000")
    Exqlite.Sqlite3.execute(db, "PRAGMA mmap_size = 134217728")
    Exqlite.Sqlite3.execute(db, "PRAGMA temp_store = memory")

    Exqlite.Sqlite3.execute(db, """
    CREATE TABLE IF NOT EXISTS blocks (
      block_id INTEGER PRIMARY KEY,
      file_path TEXT,
      byte_size INTEGER NOT NULL,
      entry_count INTEGER NOT NULL,
      ts_min INTEGER NOT NULL,
      ts_max INTEGER NOT NULL,
      data BLOB,
      format TEXT NOT NULL DEFAULT 'zstd',
      created_at INTEGER NOT NULL DEFAULT (unixepoch())
    )
    """)

    Exqlite.Sqlite3.execute(db, """
    CREATE TABLE IF NOT EXISTS block_terms (
      term TEXT NOT NULL,
      block_id INTEGER NOT NULL REFERENCES blocks(block_id),
      PRIMARY KEY (term, block_id)
    ) WITHOUT ROWID
    """)

    Exqlite.Sqlite3.execute(db, """
    CREATE TABLE IF NOT EXISTS trace_index (
      trace_id TEXT NOT NULL,
      block_id INTEGER NOT NULL REFERENCES blocks(block_id),
      span_count INTEGER NOT NULL,
      root_span_name TEXT,
      duration_ns INTEGER,
      has_error INTEGER DEFAULT 0,
      PRIMARY KEY (trace_id, block_id)
    ) WITHOUT ROWID
    """)

    Exqlite.Sqlite3.execute(db, """
    CREATE INDEX IF NOT EXISTS idx_blocks_ts ON blocks(ts_min, ts_max)
    """)

    Exqlite.Sqlite3.execute(db, """
    CREATE INDEX IF NOT EXISTS idx_trace_error ON trace_index(has_error)
    """)
  end

  # --- Indexing ---

  defp do_index_block(state, meta, terms, trace_rows) do
    Exqlite.Sqlite3.execute(state.db, "BEGIN")
    insert_block_data(state, meta, terms, trace_rows)
    Exqlite.Sqlite3.execute(state.db, "COMMIT")
    :ok
  end

  # Insert pre-computed block data (caller manages transaction)
  defp insert_block_data(state, meta, terms, trace_rows) do
    format = Map.get(meta, :format, :zstd)
    format_str = Atom.to_string(format)
    db = state.db

    Exqlite.Sqlite3.bind(state.block_insert_stmt, [
      meta.block_id,
      meta[:file_path],
      meta.byte_size,
      meta.entry_count,
      meta.ts_min,
      meta.ts_max,
      meta[:data],
      format_str
    ])

    Exqlite.Sqlite3.step(db, state.block_insert_stmt)
    Exqlite.Sqlite3.reset(state.block_insert_stmt)

    insert_terms_batch(state, terms, meta.block_id)
    insert_trace_batch(state, meta.block_id, trace_rows)
  end

  defp insert_terms_batch(_state, [], _block_id), do: :ok

  defp insert_terms_batch(state, terms, block_id) do
    db = state.db

    terms
    |> Enum.chunk_every(@terms_batch_size)
    |> Enum.each(fn batch ->
      n = length(batch)
      params = Enum.flat_map(batch, fn term -> [term, block_id] end)
      stmt = get_or_cache_terms_stmt(state, n)
      Exqlite.Sqlite3.bind(stmt, params)
      Exqlite.Sqlite3.step(db, stmt)
      Exqlite.Sqlite3.reset(stmt)
    end)
  end

  defp insert_trace_batch(_state, _block_id, []), do: :ok

  defp insert_trace_batch(state, block_id, trace_rows) do
    db = state.db

    trace_rows
    |> Enum.chunk_every(@trace_batch_size)
    |> Enum.each(fn batch ->
      n = length(batch)

      params =
        Enum.flat_map(batch, fn {trace_id, count, root_name, dur, err} ->
          [trace_id, block_id, count, root_name, dur, err]
        end)

      stmt = get_or_cache_trace_stmt(state, n)
      Exqlite.Sqlite3.bind(stmt, params)
      Exqlite.Sqlite3.step(db, stmt)
      Exqlite.Sqlite3.reset(stmt)
    end)
  end

  defp get_or_cache_terms_stmt(state, n) do
    case Map.get(state.terms_stmt_cache, n) do
      nil ->
        {:ok, stmt} = Exqlite.Sqlite3.prepare(state.db, build_terms_sql(n))
        # Store in process dictionary for this flush cycle; will be added to state
        # via the caller updating state after flush_pending
        Process.put({:terms_stmt_cache, n}, stmt)
        stmt

      stmt ->
        stmt
    end
  end

  defp get_or_cache_trace_stmt(state, n) do
    case Map.get(state.trace_stmt_cache, n) do
      nil ->
        {:ok, stmt} = Exqlite.Sqlite3.prepare(state.db, build_trace_sql(n))
        Process.put({:trace_stmt_cache, n}, stmt)
        stmt

      stmt ->
        stmt
    end
  end

  defp build_terms_sql(n) do
    placeholders =
      Enum.map_join(1..n, ", ", fn i ->
        "(?#{i * 2 - 1}, ?#{i * 2})"
      end)

    "INSERT OR IGNORE INTO block_terms (term, block_id) VALUES #{placeholders}"
  end

  defp build_trace_sql(n) do
    placeholders =
      Enum.map_join(1..n, ", ", fn i ->
        base = (i - 1) * 6
        "(?#{base + 1}, ?#{base + 2}, ?#{base + 3}, ?#{base + 4}, ?#{base + 5}, ?#{base + 6})"
      end)

    "INSERT OR REPLACE INTO trace_index (trace_id, block_id, span_count, root_span_name, duration_ns, has_error) VALUES #{placeholders}"
  end

  # --- Querying (parallel, runs in caller's process) ---

  defp do_query_parallel(block_ids, storage, pagination, search_filters) do
    start_time = System.monotonic_time()

    limit = Keyword.get(pagination, :limit, @default_limit)
    offset = Keyword.get(pagination, :offset, @default_offset)
    order = Keyword.get(pagination, :order, :desc)
    blocks_read = length(block_ids)

    all_matching =
      if storage == :disk and blocks_read > 1 do
        block_ids
        |> Task.async_stream(
          fn {_block_id, file_path, format} ->
            format_atom = to_format_atom(format)

            case TimelessTraces.Writer.read_block(file_path, format_atom) do
              {:ok, entries} ->
                entries
                |> TimelessTraces.Filter.filter(search_filters)
                |> Enum.map(&TimelessTraces.Span.from_map/1)

              {:error, reason} ->
                TimelessTraces.Telemetry.event(
                  [:timeless_traces, :block, :error],
                  %{},
                  %{file_path: file_path, reason: reason}
                )

                []
            end
          end,
          max_concurrency: System.schedulers_online(),
          ordered: false
        )
        |> Enum.flat_map(fn {:ok, entries} -> entries end)
      else
        Enum.flat_map(block_ids, fn {block_id, file_path, format} ->
          format_atom = to_format_atom(format)

          read_result =
            case storage do
              :disk -> TimelessTraces.Writer.read_block(file_path, format_atom)
              :memory -> read_block_data(block_id)
            end

          case read_result do
            {:ok, entries} ->
              entries
              |> TimelessTraces.Filter.filter(search_filters)
              |> Enum.map(&TimelessTraces.Span.from_map/1)

            {:error, reason} ->
              TimelessTraces.Telemetry.event(
                [:timeless_traces, :block, :error],
                %{},
                %{file_path: file_path, reason: reason}
              )

              []
          end
        end)
      end

    sorted =
      case order do
        :asc -> Enum.sort_by(all_matching, & &1.start_time, :asc)
        :desc -> Enum.sort_by(all_matching, & &1.start_time, :desc)
      end

    total = length(sorted)
    page = sorted |> Enum.drop(offset) |> Enum.take(limit)
    duration = System.monotonic_time() - start_time

    TimelessTraces.Telemetry.event(
      [:timeless_traces, :query, :stop],
      %{duration: duration, total: total, blocks_read: blocks_read},
      %{filters: search_filters}
    )

    {:ok,
     %TimelessTraces.Result{
       entries: page,
       total: total,
       limit: limit,
       offset: offset
     }}
  end

  defp do_trace_parallel(block_info, storage, trace_id) do
    spans =
      if storage == :disk and length(block_info) > 1 do
        block_info
        |> Task.async_stream(
          fn {_block_id, file_path, format} ->
            format_atom = to_format_atom(format)

            case TimelessTraces.Writer.read_block(file_path, format_atom) do
              {:ok, entries} ->
                entries
                |> Enum.filter(fn e -> e.trace_id == trace_id end)
                |> Enum.map(&TimelessTraces.Span.from_map/1)

              {:error, _} ->
                []
            end
          end,
          max_concurrency: System.schedulers_online(),
          ordered: false
        )
        |> Enum.flat_map(fn {:ok, entries} -> entries end)
      else
        Enum.flat_map(block_info, fn {block_id, file_path, format} ->
          format_atom = to_format_atom(format)

          read_result =
            case storage do
              :disk -> TimelessTraces.Writer.read_block(file_path, format_atom)
              :memory -> read_block_data(block_id)
            end

          case read_result do
            {:ok, entries} ->
              entries
              |> Enum.filter(fn e -> e.trace_id == trace_id end)
              |> Enum.map(&TimelessTraces.Span.from_map/1)

            {:error, _} ->
              []
          end
        end)
      end

    {:ok, Enum.sort_by(spans, & &1.start_time)}
  end

  # --- Block reading ---

  defp read_block_from_db(db, block_id) do
    {:ok, stmt} =
      Exqlite.Sqlite3.prepare(db, "SELECT data, format FROM blocks WHERE block_id = ?1")

    Exqlite.Sqlite3.bind(stmt, [block_id])

    result =
      case Exqlite.Sqlite3.step(db, stmt) do
        {:row, [data, format]} when is_binary(data) ->
          TimelessTraces.Writer.decompress_block(data, to_format_atom(format))

        {:row, [nil, _format]} ->
          {:error, :no_data}

        :done ->
          {:error, :not_found}
      end

    Exqlite.Sqlite3.release(db, stmt)
    result
  end

  # --- Compaction ---

  defp do_compact_blocks(state, old_ids, new_meta, terms, trace_rows) do
    db = state.db
    storage = state.storage

    old_file_paths =
      if storage == :disk do
        {:ok, fp_stmt} =
          Exqlite.Sqlite3.prepare(db, "SELECT file_path FROM blocks WHERE block_id = ?1")

        paths =
          Enum.flat_map(old_ids, fn id ->
            Exqlite.Sqlite3.bind(fp_stmt, [id])

            result =
              case Exqlite.Sqlite3.step(db, fp_stmt) do
                {:row, [path]} when is_binary(path) -> [path]
                _ -> []
              end

            Exqlite.Sqlite3.reset(fp_stmt)
            result
          end)

        Exqlite.Sqlite3.release(db, fp_stmt)
        paths
      else
        []
      end

    Exqlite.Sqlite3.execute(db, "BEGIN")

    {:ok, del_terms} = Exqlite.Sqlite3.prepare(db, "DELETE FROM block_terms WHERE block_id = ?1")
    {:ok, del_traces} = Exqlite.Sqlite3.prepare(db, "DELETE FROM trace_index WHERE block_id = ?1")
    {:ok, del_blocks} = Exqlite.Sqlite3.prepare(db, "DELETE FROM blocks WHERE block_id = ?1")

    for id <- old_ids do
      Exqlite.Sqlite3.bind(del_terms, [id])
      Exqlite.Sqlite3.step(db, del_terms)
      Exqlite.Sqlite3.reset(del_terms)

      Exqlite.Sqlite3.bind(del_traces, [id])
      Exqlite.Sqlite3.step(db, del_traces)
      Exqlite.Sqlite3.reset(del_traces)

      Exqlite.Sqlite3.bind(del_blocks, [id])
      Exqlite.Sqlite3.step(db, del_blocks)
      Exqlite.Sqlite3.reset(del_blocks)
    end

    Exqlite.Sqlite3.release(db, del_terms)
    Exqlite.Sqlite3.release(db, del_traces)
    Exqlite.Sqlite3.release(db, del_blocks)

    insert_block_data(state, new_meta, terms, trace_rows)

    Exqlite.Sqlite3.execute(db, "COMMIT")

    for path <- old_file_paths, do: File.rm(path)

    :ok
  end

  # --- Deletion ---

  defp do_delete_before(db, cutoff_timestamp, storage) do
    {:ok, stmt} =
      Exqlite.Sqlite3.prepare(db, "SELECT block_id, file_path FROM blocks WHERE ts_max < ?1")

    Exqlite.Sqlite3.bind(stmt, [cutoff_timestamp])
    blocks = collect_rows_2(db, stmt)
    Exqlite.Sqlite3.release(db, stmt)

    block_ids = Enum.map(blocks, fn {id, _fp} -> id end)

    # Collect terms/traces BEFORE deleting from SQLite
    old_terms = collect_terms_for_blocks(db, block_ids)
    old_traces = collect_traces_for_blocks(db, block_ids)

    count = delete_blocks(db, blocks, storage)
    {count, block_ids, old_terms, old_traces}
  end

  defp do_delete_over_size(db, max_bytes, storage) do
    {:ok, stmt} = Exqlite.Sqlite3.prepare(db, "SELECT COALESCE(SUM(byte_size), 0) FROM blocks")
    {:row, [total]} = Exqlite.Sqlite3.step(db, stmt)
    Exqlite.Sqlite3.release(db, stmt)

    if total <= max_bytes do
      {0, [], [], []}
    else
      {:ok, stmt} =
        Exqlite.Sqlite3.prepare(
          db,
          "SELECT block_id, file_path, byte_size FROM blocks ORDER BY ts_min ASC"
        )

      rows = collect_rows_with_size(db, stmt)
      Exqlite.Sqlite3.release(db, stmt)

      {to_delete, _} =
        Enum.reduce_while(rows, {[], total}, fn {block_id, file_path, size}, {acc, remaining} ->
          if remaining > max_bytes do
            {:cont, {[{block_id, file_path} | acc], remaining - size}}
          else
            {:halt, {acc, remaining}}
          end
        end)

      block_ids = Enum.map(to_delete, fn {id, _fp} -> id end)

      # Collect terms/traces BEFORE deleting from SQLite
      old_terms = collect_terms_for_blocks(db, block_ids)
      old_traces = collect_traces_for_blocks(db, block_ids)

      count = delete_blocks(db, to_delete, storage)
      {count, block_ids, old_terms, old_traces}
    end
  end

  defp delete_blocks(_db, [], _storage), do: 0

  defp delete_blocks(db, blocks, storage) do
    Exqlite.Sqlite3.execute(db, "BEGIN")

    {:ok, del_terms} = Exqlite.Sqlite3.prepare(db, "DELETE FROM block_terms WHERE block_id = ?1")
    {:ok, del_traces} = Exqlite.Sqlite3.prepare(db, "DELETE FROM trace_index WHERE block_id = ?1")
    {:ok, del_blocks} = Exqlite.Sqlite3.prepare(db, "DELETE FROM blocks WHERE block_id = ?1")

    for {block_id, file_path} <- blocks do
      Exqlite.Sqlite3.bind(del_terms, [block_id])
      Exqlite.Sqlite3.step(db, del_terms)
      Exqlite.Sqlite3.reset(del_terms)

      Exqlite.Sqlite3.bind(del_traces, [block_id])
      Exqlite.Sqlite3.step(db, del_traces)
      Exqlite.Sqlite3.reset(del_traces)

      Exqlite.Sqlite3.bind(del_blocks, [block_id])
      Exqlite.Sqlite3.step(db, del_blocks)
      Exqlite.Sqlite3.reset(del_blocks)

      if storage == :disk do
        File.rm(file_path)
      end
    end

    Exqlite.Sqlite3.release(db, del_terms)
    Exqlite.Sqlite3.release(db, del_traces)
    Exqlite.Sqlite3.release(db, del_blocks)

    Exqlite.Sqlite3.execute(db, "COMMIT")
    length(blocks)
  end

  # --- Query building ---

  defp split_pagination(filters) do
    {pagination, search} =
      Enum.split_with(filters, fn {k, _v} -> k in [:limit, :offset, :order] end)

    {search, pagination}
  end

  defp split_filters(filters) do
    term_filters =
      Enum.filter(filters, fn {k, _v} ->
        k in [:name, :kind, :status, :service, :attributes]
      end)

    time_filters =
      Enum.filter(filters, fn {k, _v} -> k in [:since, :until] end)

    {term_filters, time_filters}
  end

  defp build_query_terms(term_filters) do
    Enum.flat_map(term_filters, fn
      {:name, name} -> ["name:#{name}"]
      {:kind, kind} -> ["kind:#{kind}"]
      {:status, status} -> ["status:#{status}"]
      {:service, svc} -> ["service.name:#{svc}"]
      {:attributes, map} -> Enum.map(map, fn {k, v} -> "#{k}:#{v}" end)
      _ -> []
    end)
  end

  defp to_nanos(%DateTime{} = dt), do: DateTime.to_unix(dt, :nanosecond)
  defp to_nanos(ts) when is_integer(ts), do: ts

  defp to_format_atom("raw"), do: :raw
  defp to_format_atom("zstd"), do: :zstd
  defp to_format_atom("openzl"), do: :openzl
  defp to_format_atom(:raw), do: :raw
  defp to_format_atom(:zstd), do: :zstd
  defp to_format_atom(:openzl), do: :openzl
  defp to_format_atom(_), do: :zstd

  # --- Row collectors ---

  defp collect_single_column(db, stmt) do
    case Exqlite.Sqlite3.step(db, stmt) do
      {:row, [value]} -> [value | collect_single_column(db, stmt)]
      :done -> []
    end
  end

  defp collect_rows_2(db, stmt) do
    case Exqlite.Sqlite3.step(db, stmt) do
      {:row, [block_id, file_path]} -> [{block_id, file_path} | collect_rows_2(db, stmt)]
      :done -> []
    end
  end

  defp collect_rows_with_size(db, stmt) do
    case Exqlite.Sqlite3.step(db, stmt) do
      {:row, [block_id, file_path, size]} ->
        [{block_id, file_path, size} | collect_rows_with_size(db, stmt)]

      :done ->
        []
    end
  end
end
